{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transform, Format and Clean Data. \n",
    "\n",
    "# 2. Seperate into dimensions and facts\n",
    "\n",
    "# 3. Save the data frames as CSV  \n",
    "\n",
    "# 4. Load Data into the Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON config file\n",
    "config_file_path = 'config.json'\n",
    "with open(config_file_path, 'r') as config_file:\n",
    "    config = json.load(config_file) \n",
    "\n",
    "# Azure connection string\n",
    "CONNECTION_STRING = config['AZURE_CONNECTION_STRING']\n",
    "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\n",
    "\n",
    "# Database connection\n",
    "DATABASE = config['DW_CONNECTION_STRING']\n",
    "#engine = create_engine(DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_list(container_name):\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs()\n",
    "    return blob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azure_blob_data(container_name, blob):\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    stream = blob_client.download_blob()\n",
    "    blob_content = b\"\"\n",
    "    for chunk in stream.chunks():\n",
    "        blob_content += chunk\n",
    "    return blob_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    response = requests.get(url)\n",
    "    return io.BytesIO(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the data from Azure Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_ppp_loan_data():\\n    container_name = \\'pppdata\\'\\n    blob_list = get_blob_list(container_name)\\n    df_list = []  # Initialize df_list outside the loop\\n\\n    print(f\"Downloading data from {container_name} container\\n\")\\n    for blob in blob_list:\\n        print(f\"Downloading:\\t{blob.name}\")\\n        blob_data = get_azure_blob_data(container_name, blob)\\n        print(f\"Downloaded {blob.name} successfully\\n\")\\n        data = io.BytesIO(blob_data)\\n        print(f\"Reading:\\t{blob.name}\")\\n        df_chunks = pd.read_csv(data, chunksize=100000)  # Adjust the chunksize as per your memory capacity\\n        for chunk in df_chunks:\\n            df_list.append(chunk)\\n        print(f\"Read {blob.name} successfully\\n\\n\")\\n    \\n    if df_list:  # Check if df_list is not empty\\n        df = pd.concat(df_list)\\n        print(f\"PPP consolidated successfully\")\\n        return df\\n    else:\\n        print(\"No data downloaded.\")\\n        return None'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_ppp_loan_data():\n",
    "    container_name = 'pppdata'\n",
    "    blob_list = get_blob_list(container_name)\n",
    "    df_list = []  # Initialize df_list outside the loop\n",
    "\n",
    "    print(f\"Downloading data from {container_name} container\\n\")\n",
    "    for blob in blob_list:\n",
    "        print(f\"Downloading:\\t{blob.name}\")\n",
    "        blob_data = get_azure_blob_data(container_name, blob)\n",
    "        print(f\"Downloaded {blob.name} successfully\\n\")\n",
    "        data = io.BytesIO(blob_data)\n",
    "        print(f\"Reading:\\t{blob.name}\")\n",
    "        df_chunks = pd.read_csv(data, chunksize=100000)  # Adjust the chunksize as per your memory capacity\n",
    "        for chunk in df_chunks:\n",
    "            df_list.append(chunk)\n",
    "        print(f\"Read {blob.name} successfully\\n\\n\")\n",
    "    \n",
    "    if df_list:  # Check if df_list is not empty\n",
    "        df = pd.concat(df_list)\n",
    "        print(f\"PPP consolidated successfully\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data downloaded.\")\n",
    "        return None\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppp_loan_data():\n",
    "    container_name = 'pppdata'\n",
    "    blob_list = get_blob_list(container_name)\n",
    "    \n",
    "    for blob in blob_list:\n",
    "        if \"public_150k_plus\" in blob.name:\n",
    "            print(f\"Downloading {blob.name}\")\n",
    "            blob_data = get_azure_blob_data(container_name, blob)\n",
    "            print(f\"Downloaded {blob.name} successfully\")\n",
    "            data = io.BytesIO(blob_data)\n",
    "            print(f\"Reading {blob.name}\")\n",
    "            df_chunks = pd.read_csv(data, chunksize=100000)  # Adjust the chunksize as per your memory capacity\n",
    "            df_list = []\n",
    "            for chunk in df_chunks:\n",
    "                df_list.append(chunk)\n",
    "            df = pd.concat(df_list)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naics_data():\n",
    "    container_name = 'naicsdata'\n",
    "    blob_list = get_blob_list(container_name)\n",
    "\n",
    "    for blob in blob_list:\n",
    "        blob_data = get_azure_blob_data(container_name, blob)\n",
    "        data = io.BytesIO(blob_data)\n",
    "        df = pd.read_csv(data)\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdp_data():\n",
    "    container_name = 'gdpdata'\n",
    "    blob_list = get_blob_list(container_name)\n",
    "\n",
    "    for blob in blob_list:\n",
    "        blob_data = get_azure_blob_data(container_name, blob)\n",
    "        data = io.BytesIO(blob_data)\n",
    "        df = pd.read_csv(data)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformating, and Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_naics_data():\n",
    "    df_naics = get_naics_data()\n",
    "    df_naics.rename(columns={\n",
    "        'Code': 'naics_code',\n",
    "        'Title': 'naics_title',\n",
    "        'Description': 'description'\n",
    "    }, inplace=True)\n",
    "    # Remove all the rows where naics_code is not a number\n",
    "    # The naics_code column has some generic values like \"31-33\" which are not valid NAICS codes\n",
    "    df_naics = df_naics[df_naics['naics_code'].str.isnumeric()]\n",
    "\n",
    "    df_naics['naics_code'] = df_naics['naics_code'].astype(int)\n",
    "    df_naics['naics_title'] = df_naics['naics_title'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_naics['description'] = df_naics['description'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    \n",
    "    return df_naics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_gdp_data():\n",
    "    df_gdp  = get_gdp_data()\n",
    "    #Drop all the records where 2017, 2018, 2019, 2020, 2021, 2022 = \"(NA)\" \n",
    "    df_gdp = df_gdp[df_gdp['2017'] != \"(NA)\"]\n",
    "    df_gdp = df_gdp[df_gdp['2020'] != \"(NA)\"]\n",
    "\n",
    "    # Pivot the data in GDP data\n",
    "    selected_columns = ['GeoFIPS', 'GeoName', 'Region', 'Description', '2017', '2018', '2019', '2020', '2021', '2022']\n",
    "    df_gdp = df_gdp[selected_columns]\n",
    "    pivot_data = df_gdp.melt(id_vars=[\"GeoFIPS\", \"GeoName\", \"Region\", \"Description\"],\n",
    "                                    value_vars=[\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"],\n",
    "                                    var_name=\"date_id\",\n",
    "                                    value_name=\"Value\")\n",
    "    pivot_data = pivot_data.pivot_table(index=[\"GeoFIPS\", \"GeoName\", \"Region\", \"date_id\"], columns=\"Description\", values=\"Value\", aggfunc='first').reset_index()\n",
    "    pivot_data = pivot_data.sort_values(by=[\"GeoFIPS\", \"date_id\"])\n",
    "    pivot_data.rename(columns={\n",
    "        \"Chain-type quantity indexes for real GDP \": \"chain_type_index_gdp\",\n",
    "        \"Current-dollar GDP (thousands of current dollars) \": \"current_dollar_gdp\",\n",
    "        \"Real GDP (thousands of chained 2017 dollars) \": \"real_gdp\",\n",
    "        \"GeoFIPS\": \"geofips\",\n",
    "        \"GeoName\": \"geo_name\",\n",
    "        \"Description\": \"Index\",\n",
    "        \"date_id\": \"year_id\",\n",
    "        \"Region\": \"region\"\n",
    "    }, inplace=True)\n",
    "    pivot_data['facts_gdp_id'] = range(1, len(pivot_data) + 1)\n",
    "    final_data = pivot_data.drop(columns='Description', errors='ignore')\n",
    "    final_data = pivot_data[['facts_gdp_id', 'geofips', 'geo_name', 'region', 'year_id', 'chain_type_index_gdp',\n",
    "                         'current_dollar_gdp', 'real_gdp']]\n",
    "    df_gdp = final_data\n",
    "\n",
    "    # Remove the quation marks from geofips\n",
    "    df_gdp['geofips'] = df_gdp['geofips'].str.replace('\"', '')\n",
    "    \n",
    "    # Change the YearID to match the format in the Date Dimension\n",
    "    df_gdp['year_id'] = pd.to_datetime(df_gdp['year_id'], format='%Y').dt.strftime('%Y%m%d%H')\n",
    "    \n",
    "    # Change the data types of the columns\n",
    "    df_gdp['year_id'] = df_gdp['year_id'].astype(int)\n",
    "    df_gdp['geofips'] = df_gdp['geofips'].astype(int)\n",
    "    df_gdp['geo_name'] = df_gdp['geo_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_gdp['region'] = df_gdp['region'].astype(pd.StringDtype(\"pyarrow\"))    \n",
    "    df_gdp['chain_type_index_gdp'] = df_gdp['chain_type_index_gdp'].astype(float)\n",
    "    df_gdp['current_dollar_gdp'] = df_gdp['current_dollar_gdp'].astype(float)\n",
    "    df_gdp['real_gdp'] = df_gdp['real_gdp'].astype(float)\n",
    "\n",
    "\n",
    "    return df_gdp\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_ppp_loan_data():\n",
    "    df_ppp = get_ppp_loan_data()\n",
    "\n",
    "\n",
    "    # Delete the columns that are not required\n",
    "    df_ppp.drop(columns=[\n",
    "        'UTILITIES_PROCEED',\n",
    "        'PAYROLL_PROCEED',\n",
    "        'MORTGAGE_INTEREST_PROCEED',\n",
    "        'RENT_PROCEED',\n",
    "        'REFINANCE_EIDL_PROCEED',\n",
    "        'HEALTH_CARE_PROCEED',\n",
    "        'DEBT_INTEREST_PROCEED',\n",
    "        'RuralUrbanIndicator',\n",
    "        'HubzoneIndicator',\n",
    "        'LMIIndicator',\n",
    "        'ProjectCity',\n",
    "        'ProjectZip',\n",
    "        'CD'\n",
    "    ], inplace=True)\n",
    "    # Rename the columns to match the SQL table\n",
    "    df_ppp.rename(columns={\n",
    "        'LoanNumber': 'loan_number',\n",
    "        'DateApproved': 'date_approved_id',\n",
    "        'SBAOfficeCode': 'sba_office_code',\n",
    "        'ProcessingMethod': 'processing_method',\n",
    "        'BorrowerName': 'borrower_name',\n",
    "        'BorrowerAddress': 'borrower_address',\n",
    "        'BorrowerCity': 'borrower_city',\n",
    "        'BorrowerState': 'borrower_state',\n",
    "        'BorrowerZip': 'borrower_zip',\n",
    "        'LoanStatusDate': 'loan_status_date_id',\n",
    "        'LoanStatus': 'loan_status',\n",
    "        'Term': 'term_month',\n",
    "        'SBAGuarantyPercentage': 'sba_guaranty_percentage',\n",
    "        'InitialApprovalAmount': 'initial_approval_amount',\n",
    "        'CurrentApprovalAmount': 'current_approval_amount',\n",
    "        'UndisbursedAmount': 'undisbursed_amount',\n",
    "        'FranchiseName': 'franchise_name',\n",
    "        'ServicingLenderLocationID': 'servicing_lender_location_id',\n",
    "        'ServicingLenderName': 'servicing_lender_name',\n",
    "        'ServicingLenderAddress': 'servicing_lender_address',\n",
    "        'ServicingLenderCity': 'servicing_lender_city',\n",
    "        'ServicingLenderState': 'servicing_lender_state',\n",
    "        'ServicingLenderZip': 'servicing_lender_zip',\n",
    "        'BusinessAgeDescription': 'business_age_description',\n",
    "        'ProjectState': 'project_state',\n",
    "        'ProjectCountyName': 'project_county_name',\n",
    "        'Race': 'race',\n",
    "        'Ethnicity': 'ethnicity',\n",
    "        'Gender': 'gender',\n",
    "        'BusinessType': 'business_type',\n",
    "        'OriginatingLenderLocationID': 'originating_lender_location_id',\n",
    "        'OriginatingLender': 'originating_lender',\n",
    "        'OriginatingLenderCity': 'originating_lender_city',\n",
    "        'OriginatingLenderState': 'originating_lender_state',\n",
    "        'Veteran': 'veteran',\n",
    "        'NonProfit': 'nonprofit',\n",
    "        'ForgivenessAmount': 'forgiveness_amount',\n",
    "        'ForgivenessDate': 'forgiveness_date_id',\n",
    "        'JobsReported': 'jobs_reported',\n",
    "        'NAICSCode': 'naics_code'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Droping all the empty rows\n",
    "    # Drop all the rows where Borrower State is empty\n",
    "    df_ppp = df_ppp.dropna(subset=['borrower_state'])\n",
    "\n",
    "    # Drop all the rows where naics_code is empty\n",
    "    df_ppp = df_ppp.dropna(subset=['naics_code'])\n",
    "\n",
    "    # Drop all the rows where dates are empty\n",
    "    df_ppp = df_ppp.dropna(subset=['date_approved_id', 'loan_status_date_id', 'forgiveness_date_id'])\n",
    "\n",
    "    # Drop all the rows where jobs reported is empty\n",
    "    df_ppp = df_ppp.dropna(subset=['jobs_reported'])\n",
    "\n",
    "    # Drop all the rows where business type is empty\n",
    "    df_ppp = df_ppp.dropna(subset=['business_type'])\n",
    "\n",
    "    # Drop all the rows where business age description is empty\n",
    "    df_ppp = df_ppp.dropna(subset=['business_age_description'])\n",
    "    # or where the value is Unanswered\n",
    "    df_ppp = df_ppp[df_ppp['business_age_description'] != 'Unanswered']\n",
    "\n",
    "    \n",
    "\n",
    "    # Change the Date columns to match the format in the Date Dimension\n",
    "    df_ppp['forgiveness_date_id'] = pd.to_datetime(df_ppp['forgiveness_date_id']).dt.strftime('%Y%m%d%H')\n",
    "    df_ppp['date_approved_id'] = pd.to_datetime(df_ppp['date_approved_id']).dt.strftime('%Y%m%d%H')\n",
    "    df_ppp['loan_status_date_id'] = pd.to_datetime(df_ppp['loan_status_date_id']).dt.strftime('%Y%m%d%H')\n",
    "    \n",
    "    # Change nonprofit to boolean\n",
    "    df_ppp['nonprofit'] = df_ppp['nonprofit'].map({'Y': True})\n",
    "    df_ppp['nonprofit'] = df_ppp['nonprofit'].fillna(False)\n",
    "\n",
    "    # Change veteran to boolean\n",
    "    df_ppp['veteran'] = df_ppp['veteran'].map({'veteran': True, 'Non-veteran': False, 'Unanswered':None})\n",
    "\n",
    "    # Sentence case the string columns\n",
    "    df_ppp['borrower_address'] = df_ppp['borrower_address'].str.title()\n",
    "    df_ppp['borrower_city'] = df_ppp['borrower_city'].str.title()\n",
    "    df_ppp['originating_lender_city'] = df_ppp['originating_lender_city'].str.title()\n",
    "    df_ppp['servicing_lender_city'] = df_ppp['servicing_lender_city'].str.title()\n",
    "    df_ppp['project_county_name'] = df_ppp['project_county_name'].str.title()\n",
    "    \n",
    "    #df_ppp['loan_number'] = df_ppp['loan_number'].astype(int)\n",
    "    df_ppp['date_approved_id'] = df_ppp['date_approved_id'].astype(int)\n",
    "    df_ppp['sba_office_code'] = df_ppp['sba_office_code'].astype(int)\n",
    "    df_ppp['processing_method'] = df_ppp['processing_method'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['borrower_name'] = df_ppp['borrower_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['borrower_address'] = df_ppp['borrower_address'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['borrower_city'] = df_ppp['borrower_city'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['borrower_state'] = df_ppp['borrower_state'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['borrower_zip'] = df_ppp['borrower_zip'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['loan_status_date_id'] = df_ppp['loan_status_date_id'].astype(int)\n",
    "    df_ppp['loan_status'] = df_ppp['loan_status'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['term_month'] = df_ppp['term_month'].astype(int)\n",
    "    df_ppp['sba_guaranty_percentage'] = df_ppp['sba_guaranty_percentage'].astype(float)\n",
    "    df_ppp['initial_approval_amount'] = df_ppp['initial_approval_amount'].astype(float)\n",
    "    df_ppp['current_approval_amount'] = df_ppp['current_approval_amount'].astype(float)\n",
    "    df_ppp['undisbursed_amount'] = df_ppp['undisbursed_amount'].astype(float)\n",
    "    df_ppp['franchise_name'] = df_ppp['franchise_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['servicing_lender_location_id'] = df_ppp['servicing_lender_location_id'].astype(int)\n",
    "    df_ppp['servicing_lender_name'] = df_ppp['servicing_lender_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['servicing_lender_address'] = df_ppp['servicing_lender_address'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['servicing_lender_city'] = df_ppp['servicing_lender_city'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['servicing_lender_state'] = df_ppp['servicing_lender_state'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['servicing_lender_zip'] = df_ppp['servicing_lender_zip'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['business_age_description'] = df_ppp['business_age_description'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['project_state'] = df_ppp['project_state'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['project_county_name'] = df_ppp['project_county_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['race'] = df_ppp['race'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['ethnicity'] = df_ppp['ethnicity'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['gender'] = df_ppp['gender'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['business_type'] = df_ppp['business_type'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['originating_lender_location_id'] = df_ppp['originating_lender_location_id'].astype(int)\n",
    "    df_ppp['originating_lender'] = df_ppp['originating_lender'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['originating_lender_city'] = df_ppp['originating_lender_city'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['originating_lender_state'] = df_ppp['originating_lender_state'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "    df_ppp['veteran'] = df_ppp['veteran'].astype(bool)\n",
    "    df_ppp['nonprofit'] = df_ppp['nonprofit'].astype(bool)\n",
    "    df_ppp['forgiveness_amount'] = df_ppp['forgiveness_amount'].astype(float)\n",
    "    df_ppp['forgiveness_date_id'] = df_ppp['forgiveness_date_id'].astype(int)\n",
    "    df_ppp['jobs_reported'] = df_ppp['jobs_reported'].astype(int)\n",
    "    df_ppp['naics_code'] = df_ppp['naics_code'].astype(int)\n",
    "\n",
    "    # Create a FACTS_PPP_ID \n",
    "    df_ppp['facts_ppp_id'] = range(1, len(df_ppp) + 1)\n",
    "\n",
    "    return df_ppp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Dimensions and Facts Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading public_150k_plus_230930.csv\n",
      "Downloaded public_150k_plus_230930.csv successfully\n",
      "Reading public_150k_plus_230930.csv\n"
     ]
    }
   ],
   "source": [
    "clean_ppp_data = reformat_ppp_loan_data()\n",
    "clean_naics_data = reformat_naics_data()\n",
    "clean_gdp_data = reformat_gdp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dimensions\n",
    "dim_naics = reformat_naics_data() # Completed\n",
    "dim_naics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_sba_office = clean_ppp_data[['sba_office_code']].drop_duplicates()\n",
    "dim_sba_office = dim_sba_office.reset_index(drop=True)\n",
    "dim_sba_office.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim_geography = clean_gdp_data[['geofips', 'geo_name', 'region']].drop_duplicates()\n",
    "dim_geography = dim_geography.reset_index(drop=True)\n",
    "# Make geo_name as regular string\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].astype(str)\n",
    "\n",
    "\"\"\"\n",
    "Strip phrases from the geo_name column:\n",
    "Borough\n",
    "City and Borough\n",
    "Census Area\n",
    "Municipality\n",
    "(Independent City)\n",
    "\n",
    "Strip any asterisks from the geo_name column\n",
    "\"\"\"\n",
    "\"\"\"dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\" Borough\")\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\" City and Borough\")\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\" Census Area\")\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\" Municipality\")\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\" (Independent City)\")\n",
    "\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].str.strip(\"*\")\"\"\"\n",
    "\n",
    "#Return geo_name into a py arrow string\n",
    "dim_geography['geo_name'] = dim_geography['geo_name'].astype(pd.StringDtype(\"pyarrow\"))\n",
    "\n",
    "dim_geography['project_state'] = dim_geography['geo_name'].str.split(',').str[-1].str.strip()\n",
    "dim_geography['project_county_name'] = dim_geography['geo_name'].str.split(',').str[0].str.strip()\n",
    "\n",
    "# Temporarily set geofips to string\n",
    "dim_geography['geofips'] = dim_geography['geofips'].astype(str)\n",
    "\n",
    "# Set the project_state and project_county_name for the United States\n",
    "dim_geography.loc[dim_geography['geofips'] == '0', 'project_state'] = 'All States'\n",
    "dim_geography.loc[dim_geography['geofips'] == '0', 'project_county_name'] = 'All Counties'\n",
    "\n",
    "# Set the project_state and project_county_name for the States\n",
    "dim_geography.loc[dim_geography['geofips'].str.endswith('000'), 'project_state'] = dim_geography['geo_name']\n",
    "dim_geography.loc[dim_geography['geofips'].str.endswith('000'), 'project_county_name'] = 'All Counties'\n",
    "\n",
    "# Reset the geofips to int\n",
    "dim_geography['geofips'] = dim_geography['geofips'].astype(int)\n",
    "\n",
    "# In the clean_ppp_data, create the GEONAME column using the project_state and project_county_name\n",
    "clean_ppp_data['geo_name'] = clean_ppp_data['project_county_name'] + ', ' + clean_ppp_data['project_state']\n",
    "\n",
    "#Show me the project_state and project_county_name, geo_name head in the clean_ppp_data\n",
    "clean_ppp_data[['project_state', 'project_county_name', 'geo_name']].head()\n",
    "\n",
    "\n",
    "# Merge the clean_ppp_data with the dim_geography to get the geofips\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_geography[['geo_name', 'geofips']], on='geo_name', how='left', suffixes=('', '_dim_geography'))\n",
    "# Make geofips as int\n",
    "\n",
    "dim_geography.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_originating_lender = clean_ppp_data[['originating_lender_location_id', 'originating_lender', 'originating_lender_city', 'originating_lender_state']].drop_duplicates()\n",
    "dim_originating_lender[\"originating_lender_id\"] = range(1, len(dim_originating_lender) + 1)\n",
    "# Change column order\n",
    "dim_originating_lender = dim_originating_lender[['originating_lender_id', 'originating_lender_location_id', 'originating_lender', 'originating_lender_city', 'originating_lender_state']]\n",
    "dim_originating_lender = dim_originating_lender.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_originating_lender[['originating_lender_location_id', 'originating_lender_id']], on='originating_lender_location_id', how='left', suffixes=('', '_dim_originating_lender'))\n",
    "\n",
    "dim_originating_lender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_borrower = clean_ppp_data[['borrower_name', 'borrower_address', 'borrower_city', 'borrower_state', 'borrower_zip', 'race', 'ethnicity', 'gender', 'veteran', 'franchise_name', 'nonprofit', 'jobs_reported']].drop_duplicates()\n",
    "dim_borrower[\"borrower_id\"] = range(1, len(dim_borrower) + 1)\n",
    "# Change the column order\n",
    "dim_borrower = dim_borrower.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_borrower[['borrower_name', 'borrower_address', 'borrower_city', 'borrower_state', 'borrower_zip', 'borrower_id']], on=['borrower_name', 'borrower_address', 'borrower_city', 'borrower_state', 'borrower_zip'], how='left', suffixes=('', '_dim_borrower'))\n",
    "dim_borrower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_servicing_lender = clean_ppp_data[['servicing_lender_location_id', 'servicing_lender_name', 'servicing_lender_address', 'servicing_lender_city', 'servicing_lender_state', 'servicing_lender_zip']].drop_duplicates()\n",
    "dim_servicing_lender[\"servicing_lender_id\"] = range(1, len(dim_servicing_lender) + 1)\n",
    "# Change the column order\n",
    "dim_servicing_lender = dim_servicing_lender.reset_index(drop=True)\n",
    "dim_servicing_lender = dim_servicing_lender[['servicing_lender_id', 'servicing_lender_location_id', 'servicing_lender_name', 'servicing_lender_address', 'servicing_lender_city', 'servicing_lender_state', 'servicing_lender_zip']]\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_servicing_lender[['servicing_lender_location_id', 'servicing_lender_id']], on='servicing_lender_location_id', how='left', suffixes=('', '_dim_servicing_lender'))\n",
    "dim_servicing_lender.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Factorize to create tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_loan_status = clean_ppp_data[['loan_status']].drop_duplicates()\n",
    "dim_loan_status[\"loan_status_id\"] = range(1, len(dim_loan_status) + 1)\n",
    "dim_loan_status = dim_loan_status[['loan_status_id', 'loan_status']]\n",
    "dim_loan_status = dim_loan_status.reset_index(drop=True)\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_loan_status[['loan_status', 'loan_status_id']], on='loan_status', how='left', suffixes=('', '_dim_loan_status'))\n",
    "dim_loan_status.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_business_type = clean_ppp_data[['business_type']].drop_duplicates()\n",
    "dim_business_type[\"business_type_id\"] = range(1, len(dim_business_type) + 1)\n",
    "dim_business_type = dim_business_type[['business_type_id', 'business_type']]\n",
    "dim_business_type = dim_business_type.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_business_type[['business_type', 'business_type_id']], on='business_type', how='left', suffixes=('', '_dim_business_type'))\n",
    "dim_business_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_processing_method = clean_ppp_data[['processing_method']].drop_duplicates()\n",
    "dim_processing_method[\"processing_method_id\"] = range(1, len(dim_processing_method) + 1)\n",
    "dim_processing_method = dim_processing_method[['processing_method_id', 'processing_method']]\n",
    "dim_processing_method = dim_processing_method.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_processing_method[['processing_method', 'processing_method_id']], on='processing_method', how='left', suffixes=('', '_dim_processing_method'))\n",
    "\n",
    "dim_processing_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_term = clean_ppp_data[['term_month']].drop_duplicates()\n",
    "dim_term = dim_term.sort_values(by='term_month')\n",
    "dim_term[\"term_id\"] = range(1, len(dim_term) + 1)\n",
    "dim_term = dim_term[['term_id', 'term_month']]\n",
    "dim_term = dim_term.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_term[['term_month', 'term_id']], on='term_month', how='left', suffixes=('', '_dim_term'))\n",
    "\n",
    "dim_term.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_business_age = clean_ppp_data[['business_age_description']].drop_duplicates()\n",
    "dim_business_age[\"business_age_id\"] = range(1, len(dim_business_age) + 1)\n",
    "dim_business_age = dim_business_age[['business_age_id', 'business_age_description']]\n",
    "dim_business_age = dim_business_age.reset_index(drop=True)\n",
    "\n",
    "clean_ppp_data = clean_ppp_data.merge(dim_business_age[['business_age_description', 'business_age_id']], on='business_age_description', how='left', suffixes=('', '_dim_business_age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fact_ppp = clean_ppp_data[['facts_ppp_id', 'loan_number', 'naics_code', 'geofips', 'date_approved_id', 'loan_status_date_id', 'forgiveness_date_id', 'borrower_id', 'originating_lender_id', 'servicing_lender_id', 'term_id', 'loan_status_id', 'processing_method_id', 'sba_office_code', 'business_age_id', 'business_type_id', 'sba_guaranty_percentage', 'initial_approval_amount', 'current_approval_amount', 'undisbursed_amount', 'forgiveness_amount']]\n",
    "fact_ppp = fact_ppp.reset_index(drop=True)\n",
    "fact_ppp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Fact Table\n",
    "facts_gdp = clean_gdp_data[['facts_gdp_id', 'year_id', 'real_gdp', 'chain_type_index_gdp', 'current_dollar_gdp', 'geofips']]\n",
    "facts_gdp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Dimension\n",
    "Start date: 2017-01-01 00:00:00 \n",
    "\n",
    "2017 is the minimum year in the GDP data\n",
    "\n",
    "End date: 2023-10-1 00:00:00 \n",
    "\n",
    "October 2023 is the maximum date in the PPP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_of_month(dt):\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "\n",
    "    cal = calendar.monthcalendar(year, month)\n",
    "    week_number = (day - 1) // 7 + 1\n",
    "    return week_number\n",
    "\n",
    "start_date = pd.to_datetime('2017-01-01 00:00:00')\n",
    "end_date = pd.to_datetime('2023-10-01 00:00:00')\n",
    "# Create a DataFrame for the date dimension\n",
    "date_dimension = pd.DataFrame({'date': pd.date_range(start_date, end_date, freq='H')})\n",
    "\n",
    "# Extract attributes\n",
    "date_dimension['year_number'] = date_dimension['date'].dt.year\n",
    "date_dimension['quarter_number'] = date_dimension['date'].dt.quarter #quarter_number\n",
    "date_dimension['month_number'] = date_dimension['date'].dt.month\n",
    "date_dimension['month_name'] = date_dimension['date'].dt.strftime('%B')\n",
    "date_dimension['day_number'] = date_dimension['date'].dt.day #day_number\n",
    "date_dimension['day_name'] = date_dimension['date'].dt.strftime('%A') #day_name\n",
    "date_dimension['hour_number'] = date_dimension['date'].dt.hour #hour_number\n",
    "date_dimension['date_iso_format'] = date_dimension['date'].apply(lambda x: x.isoformat())\n",
    "date_dimension['date_id'] = date_dimension['date'].dt.strftime('%Y%m%d%H')\n",
    "\n",
    "# Add week of the month and week of the year\n",
    "date_dimension['week_of_month'] = date_dimension['date'].apply(week_of_month) #week_of_month\n",
    "date_dimension['week_of_year'] = date_dimension['date'].dt.strftime('%U') #week_of_year\n",
    "\n",
    "new_order = ['date_id', 'date_iso_format','year_number','quarter_number','month_number','day_number','hour_number','month_name','day_name','week_of_year','week_of_month']\n",
    "date_dimension = date_dimension[new_order]\n",
    "\n",
    "date_dimension.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data frames as CSV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into the Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into the database\n",
    "\"\"\"dim_naics.to_sql('dim_naics', engine, if_exists='append', index=False)\n",
    "dim_sba_office.to_sql('dim_sba_office', engine, if_exists='append', index=False)\n",
    "dim_geography.to_sql('dim_geography', engine, if_exists='append', index=False)\n",
    "dim_originating_lender.to_sql('dim_originating_lender', engine, if_exists='append', index=False)\n",
    "dim_borrower.to_sql('dim_borrower', engine, if_exists='append', index=False)\n",
    "dim_servicing_lender.to_sql('dim_servicing_lender', engine, if_exists='append', index=False)\n",
    "dim_loan_status.to_sql('dim_loan_status', engine, if_exists='append', index=False)\n",
    "dim_business_type.to_sql('dim_business_type', engine, if_exists='append', index=False)\n",
    "dim_processing_method.to_sql('dim_processing_method', engine, if_exists='append', index=False)\n",
    "dim_term.to_sql('dim_term', engine, if_exists='append', index=False)\n",
    "dim_business_age.to_sql('dim_business_age', engine, if_exists='append', index=False)\n",
    "date_dimension.to_sql('date_dimension', engine, if_exists='append', index=False)\n",
    "\n",
    "fact_ppp.to_sql('fact_ppp', engine, if_exists='append', index=False)\n",
    "facts_gdp.to_sql('facts_gdp', engine, if_exists='append', index=False)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
